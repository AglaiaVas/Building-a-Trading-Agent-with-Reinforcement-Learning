{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AglaiaVas/Building-a-Trading-Agent-with-Reinforcement-Learning/blob/main/HyperParameter_simple_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JuEKmn4_F-bJ",
      "metadata": {
        "id": "JuEKmn4_F-bJ"
      },
      "outputs": [],
      "source": [
        "import talib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8429a06f-978d-4760-87e9-35b866fa92b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8429a06f-978d-4760-87e9-35b866fa92b8",
        "outputId": "5a83ab20-9699-4174-d1c4-e54d438eb092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Current Working Directory: /content/drive/My Drive/Colab Notebooks/Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to the equivalent of G:\\My Drive\\Colab Notebooks\\Project\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Project')\n",
        "\n",
        "# Verify the current working directory\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "# Now you can access your .py files within this folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b7aef2-c851-43f1-87e8-770d3882a9c0",
      "metadata": {
        "id": "d8b7aef2-c851-43f1-87e8-770d3882a9c0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de2a1ce-0716-4381-9afa-1fa126a323d7",
      "metadata": {
        "id": "4de2a1ce-0716-4381-9afa-1fa126a323d7"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from collections import deque\n",
        "from random import sample\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "from gym.envs.registration import register\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c10382-0946-4f4d-9736-929e126b45de",
      "metadata": {
        "id": "40c10382-0946-4f4d-9736-929e126b45de"
      },
      "outputs": [],
      "source": [
        "# Set NumPy random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set PyTorch random seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# If you are using CUDA (GPU) as well, you should also set the seed for all GPUs\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)  # If you are using multi-GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4142bd13-fef9-4f12-9001-04dfa7261bbb",
      "metadata": {
        "id": "4142bd13-fef9-4f12-9001-04dfa7261bbb"
      },
      "outputs": [],
      "source": [
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d72c58-45b5-438e-a647-17a9bc984176",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26d72c58-45b5-438e-a647-17a9bc984176",
        "outputId": "2ec03c25-d683-4087-b1d0-5567ec76d8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('Using GPU')\n",
        "    # Optional: Set memory growth (PyTorch handles memory management differently,\n",
        "    # but setting device or manual memory management is common).\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print('Using CPU')\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe44d9ab-2b91-46db-aec1-c1d896ece41e",
      "metadata": {
        "id": "fe44d9ab-2b91-46db-aec1-c1d896ece41e"
      },
      "outputs": [],
      "source": [
        "results_path = Path('results', 'trading_agent')\n",
        "if not results_path.exists():\n",
        "    results_path.mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fe23b3-479e-4108-b8b7-2ab9e3ea3c8d",
      "metadata": {
        "id": "09fe23b3-479e-4108-b8b7-2ab9e3ea3c8d"
      },
      "outputs": [],
      "source": [
        "def format_time(t):\n",
        "    m_, s = divmod(t, 60)\n",
        "    h, m = divmod(m_, 60)\n",
        "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VVo5Z4h5PQvF",
      "metadata": {
        "id": "VVo5Z4h5PQvF"
      },
      "source": [
        "Now we create our gym enviroment using the classes created in Trading_Environment_Add_py  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78605a77-bfd7-4a37-b87a-4e44c3f25a1a",
      "metadata": {
        "id": "78605a77-bfd7-4a37-b87a-4e44c3f25a1a"
      },
      "outputs": [],
      "source": [
        "trading_days = 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9d6efb-ae78-4f9f-ba6c-e0336be234de",
      "metadata": {
        "id": "fa9d6efb-ae78-4f9f-ba6c-e0336be234de"
      },
      "outputs": [],
      "source": [
        "register(\n",
        "    id='trading_agent-v1',\n",
        "    entry_point='Trading_Environment_Add:TradingEnvironment',\n",
        "    max_episode_steps=trading_days\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mmcP4rftRPJO",
      "metadata": {
        "id": "mmcP4rftRPJO"
      },
      "source": [
        "Now set all the parameters of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9349d2-d47a-4362-b581-a733d7569fcc",
      "metadata": {
        "id": "5c9349d2-d47a-4362-b581-a733d7569fcc"
      },
      "outputs": [],
      "source": [
        "trading_cost_bps = 1e-5\n",
        "time_cost_bps = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1777b0f-ca44-4667-a671-12d80a860a89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "d1777b0f-ca44-4667-a671-12d80a860a89",
        "outputId": "b52bc5e5-8f0a-475b-e290-c68660b41ad9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Trading costs: 0.0010% | Time costs: 0.0100%'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "f'Trading costs: {trading_cost_bps:.4%} | Time costs: {time_cost_bps:.4%}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8683b774-5c01-426b-8dfc-40d88193d86f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8683b774-5c01-426b-8dfc-40d88193d86f",
        "outputId": "0a1e2504-df06-4843-e174-6c7204e90134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Trading_Environment_Add:Loading data for ^GSPC starting from October 2004...\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "INFO:Trading_Environment_Add:Successfully retrieved data for ^GSPC.\n",
            "INFO:Trading_Environment_Add:None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data saved to ^GSPC_raw_data.csv\n",
            "Raw data:\n",
            "                   Close      Volume          Low         High\n",
            "Date                                                         \n",
            "2004-10-01  1131.500000  1582200000  1114.579956  1131.640015\n",
            "2004-10-04  1135.170044  1534000000  1131.500000  1140.130005\n",
            "2004-10-05  1134.479980  1418400000  1132.030029  1137.869995\n",
            "2004-10-06  1142.050049  1416700000  1132.939941  1142.050049\n",
            "2004-10-07  1130.650024  1447500000  1130.500000  1142.050049\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 5004 entries, 2004-11-17 to 2024-10-04\n",
            "Data columns (total 14 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   returns    5004 non-null   float64\n",
            " 1   ret_2      5004 non-null   float64\n",
            " 2   ret_5      5004 non-null   float64\n",
            " 3   ret_10     5004 non-null   float64\n",
            " 4   ret_21     5004 non-null   float64\n",
            " 5   rsi        5004 non-null   float64\n",
            " 6   macd       5004 non-null   float64\n",
            " 7   atr        5004 non-null   float64\n",
            " 8   bb_upper   5004 non-null   float64\n",
            " 9   bb_middle  5004 non-null   float64\n",
            " 10  bb_lower   5004 non-null   float64\n",
            " 11  obv        5004 non-null   float64\n",
            " 12  stoch      5004 non-null   float64\n",
            " 13  ultosc     5004 non-null   float64\n",
            "dtypes: float64(14)\n",
            "memory usage: 586.4 KB\n",
            "Preprocessed data saved to ^GSPC_preprocessed_data.csv\n",
            "Preprocessed data with technical indicators:\n",
            "              returns     ret_2     ret_5    ret_10    ret_21       rsi  \\\n",
            "Date                                                                     \n",
            "2004-11-17  0.005538 -0.146445  0.602301  0.928148  1.378176 -0.358610   \n",
            "2004-11-18  0.001362  0.384830  0.278753  0.464955  1.400780 -0.918243   \n",
            "2004-11-19 -0.011161 -0.661869 -0.563535 -0.004603  1.082120 -0.924548   \n",
            "2004-11-22  0.005896 -0.381275 -0.308727  0.211605  1.444028 -0.955649   \n",
            "2004-11-23 -0.000255  0.305416 -0.024598  0.225314  1.458100 -1.090260   \n",
            "\n",
            "                macd       atr  bb_upper  bb_middle  bb_lower       obv  \\\n",
            "Date                                                                      \n",
            "2004-11-17  0.185001 -0.863370 -0.928515  -0.948212 -0.967570 -1.675027   \n",
            "2004-11-18  0.227302 -0.882330 -0.925994  -0.945034 -0.963697 -1.671546   \n",
            "2004-11-19  0.254123 -0.866211 -0.927007  -0.941958 -0.956267 -1.675194   \n",
            "2004-11-22  0.272952 -0.866396 -0.928894  -0.938559 -0.947235 -1.671866   \n",
            "2004-11-23  0.284429 -0.873876 -0.929891  -0.935844 -0.940566 -1.675279   \n",
            "\n",
            "               stoch    ultosc  \n",
            "Date                            \n",
            "2004-11-17  0.765443  0.265995  \n",
            "2004-11-18  0.711804  0.485960  \n",
            "2004-11-19  1.214384  0.009349  \n",
            "2004-11-22  1.024236  0.170209  \n",
            "2004-11-23  0.769868 -0.215644  \n",
            "Statistics of preprocessed data:\n",
            "            returns         ret_2         ret_5        ret_10        ret_21  \\\n",
            "count  5004.000000  5.004000e+03  5.004000e+03  5.004000e+03  5.004000e+03   \n",
            "mean      0.000391 -2.839899e-18  8.519697e-18  5.679798e-18 -1.987929e-17   \n",
            "std       0.012088  1.000100e+00  1.000100e+00  1.000100e+00  1.000100e+00   \n",
            "min      -0.119841 -8.768759e+00 -7.702508e+00 -8.079865e+00 -7.325783e+00   \n",
            "25%      -0.004085 -4.256000e-01 -4.247075e-01 -4.265447e-01 -4.383858e-01   \n",
            "50%       0.000707  4.915206e-02  7.397808e-02  1.036180e-01  1.321490e-01   \n",
            "75%       0.005720  5.014932e-01  5.246323e-01  5.371013e-01  5.690552e-01   \n",
            "max       0.115800  8.218617e+00  7.867060e+00  6.543936e+00  5.280704e+00   \n",
            "\n",
            "                rsi          macd           atr      bb_upper     bb_middle  \\\n",
            "count  5.004000e+03  5.004000e+03  5.004000e+03  5.004000e+03  5.004000e+03   \n",
            "mean   3.123889e-17 -1.135960e-17 -9.087677e-17 -4.543838e-17  4.543838e-17   \n",
            "std    1.000100e+00  1.000100e+00  1.000100e+00  1.000100e+00  1.000100e+00   \n",
            "min   -1.680735e+00 -7.897398e+00 -9.651785e-01 -1.245842e+00 -1.285284e+00   \n",
            "25%   -8.488678e-01 -3.719513e-01 -6.860668e-01 -8.188598e-01 -8.184353e-01   \n",
            "50%    8.231806e-02  5.065980e-02 -4.096615e-01 -2.836861e-01 -2.793412e-01   \n",
            "75%    9.434047e-01  4.177781e-01  4.512094e-01  5.295972e-01  5.264773e-01   \n",
            "max    1.384124e+00  2.915852e+00  5.817467e+00  2.776808e+00  2.780195e+00   \n",
            "\n",
            "          bb_lower           obv         stoch        ultosc  \n",
            "count  5004.000000  5.004000e+03  5.004000e+03  5.004000e+03  \n",
            "mean      0.000000 -9.087677e-17  1.943556e-17 -3.407879e-17  \n",
            "std       1.000100  1.000100e+00  1.000100e+00  1.000100e+00  \n",
            "min      -1.341035 -1.685169e+00 -2.596039e+00 -3.136567e+00  \n",
            "25%      -0.812978 -1.026727e+00 -6.635020e-01 -6.987028e-01  \n",
            "50%      -0.286394  1.837073e-01  1.651097e-02  4.740952e-02  \n",
            "75%       0.537482  8.144148e-01  6.587921e-01  7.257772e-01  \n",
            "max       2.789706  1.571345e+00  2.558240e+00  2.937593e+00  \n"
          ]
        }
      ],
      "source": [
        "from gym.wrappers import StepAPICompatibility\n",
        "\n",
        "# Wrap your environment with the compatibility wrapper\n",
        "trading_environment = gym.make('trading_agent-v0',\n",
        "                               ticker='^GSPC',\n",
        "                               trading_days=trading_days,\n",
        "                               trading_cost_bps=trading_cost_bps,\n",
        "                               time_cost_bps=time_cost_bps)\n",
        "\n",
        "# Add the compatibility wrapper\n",
        "trading_environment = StepAPICompatibility(trading_environment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad87e7b-cdfe-4f14-a889-f380990a366e",
      "metadata": {
        "id": "4ad87e7b-cdfe-4f14-a889-f380990a366e"
      },
      "outputs": [],
      "source": [
        "state_dim = trading_environment.observation_space.shape[0]\n",
        "num_actions = trading_environment.action_space.n\n",
        "max_episode_steps = trading_environment.spec.max_episode_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36106286-7f78-417d-8d09-6ca80bce232d",
      "metadata": {
        "id": "36106286-7f78-417d-8d09-6ca80bce232d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import logging\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "class DDQNAgent:\n",
        "    def __init__(self, state_dim,\n",
        "                 num_actions,\n",
        "                 learning_rate,\n",
        "                 gamma,\n",
        "                 epsilon_start,\n",
        "                 epsilon_end,\n",
        "                 epsilon_decay_steps,\n",
        "                 epsilon_exponential_decay,\n",
        "                 replay_capacity,\n",
        "                 architecture,\n",
        "                 l2_reg,\n",
        "                 tau,\n",
        "                 batch_size):\n",
        "        # Initialize parameters for the DDQN agent\n",
        "        self.state_dim = state_dim\n",
        "        self.num_actions = num_actions\n",
        "        self.experience = deque([], maxlen=replay_capacity)  # Replay buffer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.architecture = architecture\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        # Set device to GPU if available, otherwise use CPU\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Build the online and target networks, and move them to the correct device\n",
        "        self.online_network = self.build_model().to(self.device)\n",
        "        self.target_network = self.build_model().to(self.device)\n",
        "        self.update_target()\n",
        "\n",
        "        # Initialize epsilon for epsilon-greedy action selection\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
        "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
        "        self.epsilon_history = []\n",
        "\n",
        "        # Track metrics for episodes, steps, rewards, and training\n",
        "        self.total_steps = 0\n",
        "        self.episodes = 0\n",
        "        self.episode_length = 0\n",
        "        self.rewards_history = []\n",
        "        self.steps_per_episode = []\n",
        "        self.episode_reward = 0\n",
        "\n",
        "        # Set batch size and target network update frequency (tau)\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.losses = []\n",
        "        self.train = True\n",
        "\n",
        "        # Optimizer and loss function\n",
        "        self.optimizer = optim.Adam(self.online_network.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the neural network model based on the architecture provided.\n",
        "        The model will be a sequential feed-forward network with ReLU activation.\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        input_dim = self.state_dim\n",
        "\n",
        "        for units in self.architecture:\n",
        "            layers.append(nn.Linear(input_dim, units))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.1))  # Add dropout for regularization\n",
        "            input_dim = units\n",
        "\n",
        "        # Output layer to map to num_actions\n",
        "        layers.append(nn.Linear(input_dim, self.num_actions))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def update_target(self):\n",
        "        \"\"\"\n",
        "        Copy the weights from the online network to the target network.\n",
        "        This is done periodically to stabilize training.\n",
        "        \"\"\"\n",
        "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
        "\n",
        "    def epsilon_greedy_policy(self, state):\n",
        "        \"\"\"\n",
        "        Select an action using epsilon-greedy policy.\n",
        "        With probability epsilon, choose a random action (exploration).\n",
        "        Otherwise, choose the action with the highest Q-value (exploitation).\n",
        "        \"\"\"\n",
        "        self.total_steps += 1\n",
        "\n",
        "        # Explore: select a random action\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.choice(range(self.num_actions))  # Random action (exploration)\n",
        "\n",
        "        # Exploit: select the action with the highest Q-value\n",
        "        state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Get Q-values from the online network\n",
        "        with torch.no_grad():  # No gradient computation during inference\n",
        "            q_values = self.online_network(state_tensor).cpu().numpy()  # Get Q-values and convert to NumPy\n",
        "\n",
        "        # Select the action with the highest Q-value\n",
        "        action = np.argmax(q_values)  # Get index of the max Q-value\n",
        "        return action  # Return the action as an integer\n",
        "\n",
        "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
        "        \"\"\"\n",
        "        Store a transition (s, a, r, s') in the replay buffer.\n",
        "        If the episode ends (done), handle epsilon decay and reset episode metrics.\n",
        "        \"\"\"\n",
        "        if not_done:\n",
        "            self.episode_reward += r\n",
        "            self.episode_length += 1\n",
        "        else:\n",
        "            if self.train:\n",
        "                if self.episodes < self.epsilon_decay_steps:\n",
        "                    self.epsilon -= self.epsilon_decay\n",
        "                else:\n",
        "                    self.epsilon *= self.epsilon_exponential_decay\n",
        "\n",
        "            self.episodes += 1\n",
        "            self.rewards_history.append(self.episode_reward)\n",
        "            self.steps_per_episode.append(self.episode_length)\n",
        "            self.episode_reward, self.episode_length = 0, 0\n",
        "\n",
        "        self.experience.append((s, a, r, s_prime, not_done))\n",
        "\n",
        "    def experience_replay(self):\n",
        "        \"\"\"\n",
        "        Train the online network using experience replay.\n",
        "        Sample a random batch of transitions from the replay buffer and update the network.\n",
        "        \"\"\"\n",
        "        if len(self.experience) < self.batch_size:\n",
        "            logging.debug(\"Not enough experiences to sample.\")\n",
        "            return\n",
        "\n",
        "        # Sample a minibatch from the replay buffer\n",
        "        minibatch = random.sample(self.experience, self.batch_size)\n",
        "        states, actions, rewards, next_states, not_done = zip(*minibatch)\n",
        "\n",
        "        # Convert experience data into PyTorch tensors and move them to the correct device\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device).view(-1)  # Shape: [batch_size]\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        not_done = torch.FloatTensor(not_done).to(self.device).view(-1)  # Shape: [batch_size]\n",
        "\n",
        "        # Debugging statements to check shapes\n",
        "        logging.debug(\"Shapes before target calculation:\")\n",
        "        logging.debug(f\"rewards: {rewards.shape}, not_done: {not_done.shape}\")\n",
        "\n",
        "        # Compute Q-values for the next states using the online network\n",
        "        next_q_values = self.online_network(next_states)\n",
        "\n",
        "        # Get the best actions for the next states\n",
        "        best_actions = next_q_values.argmax(dim=1)\n",
        "\n",
        "        # Compute Q-values for the next states using the target network\n",
        "        next_q_values_target = self.target_network(next_states)\n",
        "\n",
        "        # Gather target Q-values using the actions from next states\n",
        "        target_q_values = next_q_values_target.gather(1, best_actions.unsqueeze(1)).squeeze(1)  # Shape: [batch_size]\n",
        "\n",
        "        # Debugging statement to check the shape of target_q_values\n",
        "        logging.debug(f\"target_q_values shape: {target_q_values.shape}\")\n",
        "\n",
        "        # Compute the target Q-value: r + gamma * max(Q(s', a'))\n",
        "        targets = rewards + not_done * self.gamma * target_q_values\n",
        "\n",
        "        # Compute Q-values for the current states using the online network\n",
        "        q_values = self.online_network(states)\n",
        "\n",
        "        # Gather the Q-values corresponding to the actions taken\n",
        "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1) # Select Q-values for actions taken\n",
        "\n",
        "\n",
        "        # Compute the loss (difference between predicted and target Q-values)\n",
        "        loss = self.criterion(q_values, targets.detach())\n",
        "\n",
        "        # Backpropagate the loss and\n",
        "        # Backpropagate the loss and update the network weights\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Track the loss for analysis\n",
        "        self.losses.append(loss.item())\n",
        "\n",
        "        # Periodically update the target network based on tau\n",
        "        if self.total_steps % self.tau == 0:\n",
        "            self.update_target()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aOMuo8AVLtm",
      "metadata": {
        "id": "4aOMuo8AVLtm"
      },
      "outputs": [],
      "source": [
        "gamma = .99  # discount factor\n",
        "tau = 100  # target network update frequency\n",
        "#NN Architecture\n",
        "architecture = (256, 256)  # units per layer\n",
        "learning_rate = 0.0001  # learning rate\n",
        "l2_reg = 1e-6  # L2 regularization\n",
        "#Experience Replay\n",
        "replay_capacity = int(1e6)\n",
        "batch_size = 4096\n",
        "#greedy Policy\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = .01\n",
        "epsilon_decay_steps = 250\n",
        "epsilon_exponential_decay = .99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6tzwra2KVfGf",
      "metadata": {
        "id": "6tzwra2KVfGf"
      },
      "outputs": [],
      "source": [
        "ddqn = DDQNAgent(state_dim=state_dim,\n",
        "                 num_actions=num_actions,\n",
        "                 learning_rate=learning_rate,\n",
        "                 gamma=gamma,\n",
        "                 epsilon_start=epsilon_start,\n",
        "                 epsilon_end=epsilon_end,\n",
        "                 epsilon_decay_steps=epsilon_decay_steps,\n",
        "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
        "                 replay_capacity=replay_capacity,\n",
        "                 architecture=architecture,\n",
        "                 l2_reg=l2_reg,\n",
        "                 tau=tau,\n",
        "                 batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZU9c1hndVuhf",
      "metadata": {
        "id": "ZU9c1hndVuhf"
      },
      "outputs": [],
      "source": [
        "total_steps = 0\n",
        "max_episodes = 10\n",
        "#Initialize variables\n",
        "episode_time, navs, market_navs, diffs, episode_eps = [], [], [], [], []\n",
        "#Visualization\n",
        "def track_results(episode, nav_ma_100, nav_ma_10,\n",
        "                  market_nav_100, market_nav_10,\n",
        "                  win_ratio, total, epsilon):\n",
        "    time_ma = np.mean([episode_time[-100:]])\n",
        "    T = np.sum(episode_time)\n",
        "\n",
        "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
        "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
        "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
        "    print(template.format(episode, format_time(total),\n",
        "                          nav_ma_100-1, nav_ma_10-1,\n",
        "                          market_nav_100-1, market_nav_10-1,\n",
        "                          win_ratio, epsilon))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qxsMrk0MVnYg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxsMrk0MVnYg",
        "collapsed": true,
        "outputId": "4a7ce594-3d97-4664-d779-ee160143ec6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10 | 00:00:01 | Agent:  -4.1% ( -4.1%) | Market:   6.3% (  6.3%) | Wins: 20.0% | eps:  0.960\n"
          ]
        }
      ],
      "source": [
        "# Train Agent\n",
        "start = time()\n",
        "results = []\n",
        "for episode in range(1, max_episodes + 1):\n",
        "    this_state = trading_environment.reset()\n",
        "    logging.debug(f\"Initial state: {this_state}\")\n",
        "\n",
        "    for episode_step in range(max_episode_steps):\n",
        "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
        "        logging.debug(f\"Action taken: {action}\")\n",
        "\n",
        "        next_state, reward, done, _ = trading_environment.step(action)\n",
        "        logging.debug(f\"Next state: {next_state}, Reward: {reward}, Done: {done}\")\n",
        "\n",
        "        ddqn.memorize_transition(this_state,\n",
        "                                 action,\n",
        "                                 reward,\n",
        "                                 next_state,\n",
        "                                 0.0 if done else 1.0)\n",
        "        if ddqn.train:\n",
        "            ddqn.experience_replay()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        this_state = next_state\n",
        "\n",
        "    # get DataFrame with sequence of actions, returns, and nav values\n",
        "    result = trading_environment.env.simulator.results()\n",
        "\n",
        "    # get results of last step\n",
        "    final = result.iloc[-2]\n",
        "    logging.debug(f\"Final result of episode {episode}: {final}\")\n",
        "\n",
        "    # apply return (net of cost) of last action to last starting nav\n",
        "    nav = final.nav * (1 + final.strategy_return)\n",
        "    navs.append(nav)\n",
        "\n",
        "    # market nav\n",
        "    market_nav = final.market_nav\n",
        "    market_navs.append(market_nav)\n",
        "\n",
        "    # track difference between agent and market NAV results\n",
        "    diff = nav - market_nav\n",
        "    diffs.append(diff)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        track_results(episode,\n",
        "                      np.mean(navs[-100:]),\n",
        "                      np.mean(navs[-10:]),\n",
        "                      np.mean(market_navs[-100:]),\n",
        "                      np.mean(market_navs[-10:]),\n",
        "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
        "                      time() - start, ddqn.epsilon)\n",
        "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
        "        print(result.tail())\n",
        "        break\n",
        "\n",
        "trading_environment.close()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that all lists are the same length as the episode count\n",
        "min_length = min(len(navs), len(market_navs), len(diffs), episode)\n",
        "\n",
        "# Truncate lists to match the smallest length if necessary\n",
        "navs = navs[:min_length]\n",
        "market_navs = market_navs[:min_length]\n",
        "diffs = diffs[:min_length]\n",
        "\n",
        "# Construct the DataFrame\n",
        "final_results = pd.DataFrame({\n",
        "    'Episode': list(range(1, min_length + 1)),\n",
        "    'Agent': navs,\n",
        "    'Market': market_navs,\n",
        "    'Difference': diffs\n",
        "}).set_index('Episode')"
      ],
      "metadata": {
        "id": "hoJjK2NulDUR"
      },
      "id": "hoJjK2NulDUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the rolling mean over the last 100 episodes for both Agent and Market NAVs, subtracting 1 and showing as percentage\n",
        "final_results['Agent_rolling_mean'] = (final_results['Agent'].rolling(window=100).mean() - 1) * 100\n",
        "final_results['Market_rolling_mean'] = (final_results['Market'].rolling(window=100).mean() - 1) * 100\n",
        "\n",
        "# Compute the rolling win ratio over the last 100 points, where the agent outperformed the market\n",
        "final_results['Wins_rolling_mean'] = (final_results['Difference'] > 0).rolling(window=100).mean()"
      ],
      "metadata": {
        "id": "avlRCqv9BBdh"
      },
      "id": "avlRCqv9BBdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J5ANTS2UGH2r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5ANTS2UGH2r",
        "outputId": "a327de7e-60fb-433c-ae2e-d3c9c549c444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4035    0  4035    0     0  17306      0 --:--:-- --:--:-- --:--:-- 17317\n",
            "100  517k  100  517k    0     0   823k      0 --:--:-- --:--:-- --:--:--  823k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4031    0  4031    0     0  10214      0 --:--:-- --:--:-- --:--:-- 10230\n",
            "100  392k  100  392k    0     0   504k      0 --:--:-- --:--:-- --:--:--  504k\n"
          ]
        }
      ],
      "source": [
        "#Only uncomment the below if you need to install the Talib library\n",
        "\n",
        "\n",
        "url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'\n",
        "!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1\n",
        "url = 'https://anaconda.org/conda-forge/ta-lib/0.4.19/download/linux-64/ta-lib-0.4.19-py310hde88566_4.tar.bz2'\n",
        "!curl -L $url | tar xj -C /usr/local/lib/python3.10/dist-packages/ lib/python3.10/site-packages/talib --strip-components=3\n",
        "import talib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Initialize your actual trading environment (replace with your actual environment)\n",
        "trading_environment = gym.make('CartPole-v1')  # Replace with your trading environment\n",
        "\n",
        "state_dim = trading_environment.observation_space.shape[0]\n",
        "num_actions = trading_environment.action_space.n\n",
        "max_episode_steps = trading_environment.spec.max_episode_steps\n",
        "replay_capacity = int(1e6)  # Convert the replay capacity to an integer\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'gamma': [0.9, 0.95, 0.99],\n",
        "    'epsilon_decay': [0.99, 0.995, 0.999]\n",
        "}\n",
        "\n",
        "# Create the grid of hyperparameter combinations\n",
        "grid = ParameterGrid(param_grid)\n",
        "\n",
        "# Function to train the agent and evaluate performance\n",
        "def train_and_evaluate(env, learning_rate, gamma, epsilon_decay, episodes=100):\n",
        "    # Initialize your DDQN agent with the current hyperparameters\n",
        "    agent = DDQNAgent(state_dim, num_actions,\n",
        "                      gamma=gamma, tau=100, architecture=(256, 256),\n",
        "                      learning_rate=learning_rate, l2_reg=1e-6,\n",
        "                      replay_capacity=replay_capacity, batch_size=4096,\n",
        "                      epsilon_start=1.0, epsilon_end=0.01,\n",
        "                      epsilon_decay_steps=250, epsilon_exponential_decay=epsilon_decay)\n",
        "\n",
        "    # Run the agent for 10 episodes\n",
        "    total_rewards = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()  # Reset the environment at the start of each episode\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # The agent selects an action based on the current state\n",
        "            action = agent.epsilon_greedy_policy(state)\n",
        "\n",
        "            # Take the action in the environment, observe next state, reward, and done flag\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store the transition in memory and train the agent\n",
        "            agent.memorize_transition(state, action, reward, next_state, not done)\n",
        "            agent.experience_replay()\n",
        "\n",
        "            # Update the state\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "    # Return the average reward over all episodes\n",
        "    return np.mean(total_rewards)\n",
        "\n",
        "# Perform the hyperparameter tuning\n",
        "best_params = None\n",
        "best_reward = -np.inf\n",
        "\n",
        "for params in grid:\n",
        "    print(f\"Testing params: {params}\")\n",
        "    avg_reward = train_and_evaluate(trading_environment, params['learning_rate'], params['gamma'], params['epsilon_decay'], episodes=100)\n",
        "\n",
        "    print(f\"Average Reward: {avg_reward}\")\n",
        "\n",
        "    # Track the best-performing set of hyperparameters\n",
        "    if avg_reward > best_reward:\n",
        "        best_reward = avg_reward\n",
        "        best_params = params\n",
        "\n",
        "# Output the best set of hyperparameters and its performance\n",
        "print(f\"Best Hyperparameters: {best_params} with reward {best_reward}\")\n"
      ],
      "metadata": {
        "id": "nrCvv_F7_pg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83e2347-1053-481f-97e7-a7fa96542686"
      },
      "id": "nrCvv_F7_pg6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.9, 'learning_rate': 0.0001}\n",
            "Average Reward: 23.0\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.9, 'learning_rate': 0.001}\n",
            "Average Reward: 22.36\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.9, 'learning_rate': 0.01}\n",
            "Average Reward: 18.82\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.95, 'learning_rate': 0.0001}\n",
            "Average Reward: 17.67\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.95, 'learning_rate': 0.001}\n",
            "Average Reward: 21.52\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.95, 'learning_rate': 0.01}\n",
            "Average Reward: 19.81\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.99, 'learning_rate': 0.0001}\n",
            "Average Reward: 21.58\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.99, 'learning_rate': 0.001}\n",
            "Average Reward: 26.03\n",
            "Testing params: {'epsilon_decay': 0.99, 'gamma': 0.99, 'learning_rate': 0.01}\n",
            "Average Reward: 19.78\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.9, 'learning_rate': 0.0001}\n",
            "Average Reward: 21.22\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.9, 'learning_rate': 0.001}\n",
            "Average Reward: 24.76\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.9, 'learning_rate': 0.01}\n",
            "Average Reward: 21.79\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.95, 'learning_rate': 0.0001}\n",
            "Average Reward: 25.85\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.95, 'learning_rate': 0.001}\n",
            "Average Reward: 25.5\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.95, 'learning_rate': 0.01}\n",
            "Average Reward: 19.72\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.99, 'learning_rate': 0.0001}\n",
            "Average Reward: 20.95\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.99, 'learning_rate': 0.001}\n",
            "Average Reward: 19.97\n",
            "Testing params: {'epsilon_decay': 0.995, 'gamma': 0.99, 'learning_rate': 0.01}\n",
            "Average Reward: 17.99\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.9, 'learning_rate': 0.0001}\n",
            "Average Reward: 19.23\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.9, 'learning_rate': 0.001}\n",
            "Average Reward: 22.38\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.9, 'learning_rate': 0.01}\n",
            "Average Reward: 18.06\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.95, 'learning_rate': 0.0001}\n",
            "Average Reward: 22.9\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.95, 'learning_rate': 0.001}\n",
            "Average Reward: 21.98\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.95, 'learning_rate': 0.01}\n",
            "Average Reward: 21.86\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.99, 'learning_rate': 0.0001}\n",
            "Average Reward: 18.73\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.99, 'learning_rate': 0.001}\n",
            "Average Reward: 17.57\n",
            "Testing params: {'epsilon_decay': 0.999, 'gamma': 0.99, 'learning_rate': 0.01}\n",
            "Average Reward: 22.12\n",
            "Best Hyperparameters: {'epsilon_decay': 0.99, 'gamma': 0.99, 'learning_rate': 0.001} with reward 26.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QQ9Kv-E-TrBD"
      },
      "id": "QQ9Kv-E-TrBD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}