{"cells":[{"cell_type":"markdown","source":["# Double Deep Q-Network (DDQN) for building a Trading Agent\n","### Introduction\n","\n","In the code, **Double Deep Q-Network (DDQN)** is used to train an agent that interacts with a trading environment. The agent selects actions (buy, sell, hold) based on market data and seeks to maximize cumulative rewards. DDQN helps reduce **overestimation bias** by using two networks: the **online network** for selecting actions and the **target network** for evaluating them.\n","\n","---\n","\n","### Key Concepts in the Code\n","\n","1. **Q-Learning**:  \n","   The agent learns to maximize cumulative rewards by updating Q-values for state-action pairs. The Q-value update follows the **Bellman Equation**:\n","\n","   $$\n","   Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n","   $$\n","\n","   Where:\n","   - $Q(s, a)$: Q-value for action $a$ in state $s$\n","   - $\\alpha$: Learning rate\n","   - $r$: Reward for action $a$\n","   - $\\gamma$: Discount factor for future rewards\n","   - $s'$: The next state\n","   - $\\max_{a'} Q(s', a')$: Maximum Q-value for the next state\n","\n","2. **Double DQN**:  \n","   The code uses two networks to decouple action selection and evaluation:\n","   - **Online Network** ($Q_{\\text{online}}(s, a)$): Used to select the action with the highest Q-value.\n","   - **Target Network** ($Q_{\\text{target}}(s, a)$): Used to evaluate the value of the chosen action.\n","\n","   The DDQN update rule is:\n","\n","   $$\n","   Q_{\\text{online}}(s, a) \\leftarrow Q_{\\text{online}}(s, a) + \\alpha \\left( r + \\gamma Q_{\\text{target}}(s', \\arg\\max_{a'} Q_{\\text{online}}(s', a')) - Q_{\\text{online}}(s, a) \\right)\n","   $$\n","\n","   This reduces overestimation by separating the action selection (via the online network) from value evaluation (via the target network).\n","\n","3. **Experience Replay**:  \n","   The agent stores experiences $(s, a, r, s')$ in a replay buffer. During training, random mini-batches are sampled from the buffer to break correlations between consecutive experiences, improving learning stability.\n","\n","4. **Epsilon-Greedy Policy**:  \n","   The agent balances exploration and exploitation using an **epsilon-greedy policy**. With probability $\\epsilon$, a random action is chosen (exploration). With probability $1 - \\epsilon$, the agent selects the action with the highest Q-value (exploitation):\n","\n","   $$\n","   \\pi(a|s) =\n","   \\begin{cases}\n","   \\text{random action} & \\text{with probability } \\epsilon \\\\\n","   \\arg\\max_a Q_{\\text{online}}(s, a) & \\text{with probability } 1 - \\epsilon\n","   \\end{cases}\n","   $$\n","\n","   The epsilon value decays over time, gradually reducing exploration.\n","\n","5. **Target Network Update**:  \n","   Periodically, the target network is updated by copying the weights from the online network:\n","\n","   $$\n","   Q_{\\text{target}}(s, a) \\leftarrow Q_{\\text{online}}(s, a)\n","   $$\n","\n","   This keeps the target network stable, which helps prevent oscillations during training.\n","\n","\n"],"metadata":{"id":"f5-_vocc8kbb"},"id":"f5-_vocc8kbb"},{"cell_type":"code","execution_count":71,"id":"JuEKmn4_F-bJ","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728242308081,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"JuEKmn4_F-bJ"},"outputs":[],"source":["import talib"]},{"cell_type":"code","execution_count":72,"id":"8429a06f-978d-4760-87e9-35b866fa92b8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1527,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"8429a06f-978d-4760-87e9-35b866fa92b8","outputId":"27b3dc5a-eae8-42f5-8192-a536df9d145b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Current Working Directory: /content/drive/My Drive/Colab Notebooks/Project\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Change directory to the equivalent of G:\\My Drive\\Colab Notebooks\\Project\n","os.chdir('/content/drive/My Drive/Colab Notebooks/Project')\n","\n","# Verify the current working directory\n","print(\"Current Working Directory:\", os.getcwd())\n","\n","# Now you can access your .py files within this folder\n"]},{"cell_type":"code","execution_count":73,"id":"d8b7aef2-c851-43f1-87e8-770d3882a9c0","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"d8b7aef2-c851-43f1-87e8-770d3882a9c0"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":74,"id":"4de2a1ce-0716-4381-9afa-1fa126a323d7","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"4de2a1ce-0716-4381-9afa-1fa126a323d7"},"outputs":[],"source":["%matplotlib inline\n","from pathlib import Path\n","from time import time\n","from collections import deque\n","from random import sample\n","\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import FuncFormatter\n","import seaborn as sns\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import gym\n","from gym.envs.registration import register\n"]},{"cell_type":"code","execution_count":75,"id":"40c10382-0946-4f4d-9736-929e126b45de","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"40c10382-0946-4f4d-9736-929e126b45de"},"outputs":[],"source":["# Set NumPy random seed\n","np.random.seed(42)\n","\n","# Set PyTorch random seed\n","torch.manual_seed(42)\n","\n","# If you are using CUDA (GPU) as well, you should also set the seed for all GPUs\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)  # If you are using multi-GPU"]},{"cell_type":"code","execution_count":76,"id":"4142bd13-fef9-4f12-9001-04dfa7261bbb","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"4142bd13-fef9-4f12-9001-04dfa7261bbb"},"outputs":[],"source":["sns.set_style('whitegrid')"]},{"cell_type":"code","execution_count":77,"id":"26d72c58-45b5-438e-a647-17a9bc984176","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"26d72c58-45b5-438e-a647-17a9bc984176","outputId":"87f785d8-6b33-4a4e-8cd2-cdeb087a359c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU\n"]}],"source":["if torch.cuda.is_available():\n","    print('Using GPU')\n","    # Optional: Set memory growth (PyTorch handles memory management differently,\n","    # but setting device or manual memory management is common).\n","    device = torch.device('cuda')\n","else:\n","    print('Using CPU')\n","    device = torch.device('cpu')"]},{"cell_type":"code","execution_count":78,"id":"fe44d9ab-2b91-46db-aec1-c1d896ece41e","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"fe44d9ab-2b91-46db-aec1-c1d896ece41e"},"outputs":[],"source":["results_path = Path('results', 'trading_agent')\n","if not results_path.exists():\n","    results_path.mkdir(parents=True)"]},{"cell_type":"code","execution_count":79,"id":"09fe23b3-479e-4108-b8b7-2ab9e3ea3c8d","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"09fe23b3-479e-4108-b8b7-2ab9e3ea3c8d"},"outputs":[],"source":["def format_time(t):\n","    m_, s = divmod(t, 60)\n","    h, m = divmod(m_, 60)\n","    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"]},{"cell_type":"markdown","id":"VVo5Z4h5PQvF","metadata":{"id":"VVo5Z4h5PQvF"},"source":["Now we create our gym enviroment using the classes created in Trading_Environment_Add_py  "]},{"cell_type":"code","execution_count":80,"id":"78605a77-bfd7-4a37-b87a-4e44c3f25a1a","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"78605a77-bfd7-4a37-b87a-4e44c3f25a1a"},"outputs":[],"source":["trading_days = 252"]},{"cell_type":"code","execution_count":81,"id":"fa9d6efb-ae78-4f9f-ba6c-e0336be234de","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1728242309600,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"fa9d6efb-ae78-4f9f-ba6c-e0336be234de"},"outputs":[],"source":["register(\n","    id='trading_agent-v0',\n","    entry_point='Trading_Environment_Add:TradingEnvironment',\n","    max_episode_steps=trading_days\n",")\n"]},{"cell_type":"markdown","id":"mmcP4rftRPJO","metadata":{"id":"mmcP4rftRPJO"},"source":["Now set all the parameters of the model"]},{"cell_type":"code","execution_count":82,"id":"5c9349d2-d47a-4362-b581-a733d7569fcc","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728242309601,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"5c9349d2-d47a-4362-b581-a733d7569fcc"},"outputs":[],"source":["trading_cost_bps = 1e-5\n","time_cost_bps = 1e-4"]},{"cell_type":"code","execution_count":83,"id":"d1777b0f-ca44-4667-a671-12d80a860a89","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728242309601,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"d1777b0f-ca44-4667-a671-12d80a860a89","outputId":"2bf6a2ae-c50d-4a7d-a974-34e591ae9afe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Trading costs: 0.0010% | Time costs: 0.0100%'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":83}],"source":["f'Trading costs: {trading_cost_bps:.4%} | Time costs: {time_cost_bps:.4%}'"]},{"cell_type":"markdown","source":["# Creating the Custom Trading Environment\n","\n","In this part of the code, we are using `gym.make()` to create a custom trading environment called `trading_agent-v0`. This environment simulates trading on the S&P 500 (ticker: `^GSPC`). The environment is configured with the following parameters:\n","\n","- **trading_days**: The number of trading days per episode.\n","- **trading_cost_bps**: The transaction cost per trade, expressed in basis points (bps).\n","- **time_cost_bps**: The holding cost per time step (e.g., overnight fees), also in basis points.\n","\n","This environment setup allows reinforcement learning agents to interact with market data and make trading decisions under realistic cost conditions."],"metadata":{"id":"fgPVQtoB9qhw"},"id":"fgPVQtoB9qhw"},{"cell_type":"code","execution_count":84,"id":"8683b774-5c01-426b-8dfc-40d88193d86f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":997,"status":"ok","timestamp":1728242310590,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"8683b774-5c01-426b-8dfc-40d88193d86f","outputId":"482085e9-a1c4-439d-eda7-fb114ab7e733"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:Trading_Environment_Add:Loading data for ^GSPC starting from October 2004...\n","[*********************100%***********************]  1 of 1 completed\n","INFO:Trading_Environment_Add:Successfully retrieved data for ^GSPC.\n","INFO:Trading_Environment_Add:None\n"]},{"output_type":"stream","name":"stdout","text":["Raw data saved to ^GSPC_raw_data.csv\n","Raw data:\n","                   Close      Volume          Low         High\n","Date                                                         \n","2004-10-01  1131.500000  1582200000  1114.579956  1131.640015\n","2004-10-04  1135.170044  1534000000  1131.500000  1140.130005\n","2004-10-05  1134.479980  1418400000  1132.030029  1137.869995\n","2004-10-06  1142.050049  1416700000  1132.939941  1142.050049\n","2004-10-07  1130.650024  1447500000  1130.500000  1142.050049\n","<class 'pandas.core.frame.DataFrame'>\n","DatetimeIndex: 5004 entries, 2004-11-17 to 2024-10-04\n","Data columns (total 14 columns):\n"," #   Column     Non-Null Count  Dtype  \n","---  ------     --------------  -----  \n"," 0   returns    5004 non-null   float64\n"," 1   ret_2      5004 non-null   float64\n"," 2   ret_5      5004 non-null   float64\n"," 3   ret_10     5004 non-null   float64\n"," 4   ret_21     5004 non-null   float64\n"," 5   rsi        5004 non-null   float64\n"," 6   macd       5004 non-null   float64\n"," 7   atr        5004 non-null   float64\n"," 8   bb_upper   5004 non-null   float64\n"," 9   bb_middle  5004 non-null   float64\n"," 10  bb_lower   5004 non-null   float64\n"," 11  obv        5004 non-null   float64\n"," 12  stoch      5004 non-null   float64\n"," 13  ultosc     5004 non-null   float64\n","dtypes: float64(14)\n","memory usage: 586.4 KB\n","Preprocessed data saved to ^GSPC_preprocessed_data.csv\n","Preprocessed data with technical indicators:\n","              returns     ret_2     ret_5    ret_10    ret_21       rsi  \\\n","Date                                                                     \n","2004-11-17  0.005538 -0.146445  0.602301  0.928148  1.378176 -0.358610   \n","2004-11-18  0.001362  0.384830  0.278753  0.464955  1.400780 -0.918243   \n","2004-11-19 -0.011161 -0.661869 -0.563535 -0.004603  1.082120 -0.924548   \n","2004-11-22  0.005896 -0.381275 -0.308727  0.211605  1.444028 -0.955649   \n","2004-11-23 -0.000255  0.305416 -0.024598  0.225314  1.458100 -1.090260   \n","\n","                macd       atr  bb_upper  bb_middle  bb_lower       obv  \\\n","Date                                                                      \n","2004-11-17  0.185001 -0.863370 -0.928515  -0.948212 -0.967570 -1.675027   \n","2004-11-18  0.227302 -0.882330 -0.925994  -0.945034 -0.963697 -1.671546   \n","2004-11-19  0.254123 -0.866211 -0.927007  -0.941958 -0.956267 -1.675194   \n","2004-11-22  0.272952 -0.866396 -0.928894  -0.938559 -0.947235 -1.671866   \n","2004-11-23  0.284429 -0.873876 -0.929891  -0.935844 -0.940566 -1.675279   \n","\n","               stoch    ultosc  \n","Date                            \n","2004-11-17  0.765443  0.265995  \n","2004-11-18  0.711804  0.485960  \n","2004-11-19  1.214384  0.009349  \n","2004-11-22  1.024236  0.170209  \n","2004-11-23  0.769868 -0.215644  \n","Statistics of preprocessed data:\n","            returns         ret_2         ret_5        ret_10        ret_21  \\\n","count  5004.000000  5.004000e+03  5.004000e+03  5.004000e+03  5.004000e+03   \n","mean      0.000391 -2.839899e-18  8.519697e-18  5.679798e-18 -1.987929e-17   \n","std       0.012088  1.000100e+00  1.000100e+00  1.000100e+00  1.000100e+00   \n","min      -0.119841 -8.768759e+00 -7.702508e+00 -8.079865e+00 -7.325783e+00   \n","25%      -0.004085 -4.256000e-01 -4.247075e-01 -4.265447e-01 -4.383858e-01   \n","50%       0.000707  4.915206e-02  7.397808e-02  1.036180e-01  1.321490e-01   \n","75%       0.005720  5.014932e-01  5.246323e-01  5.371013e-01  5.690552e-01   \n","max       0.115800  8.218617e+00  7.867060e+00  6.543936e+00  5.280704e+00   \n","\n","                rsi          macd           atr      bb_upper     bb_middle  \\\n","count  5.004000e+03  5.004000e+03  5.004000e+03  5.004000e+03  5.004000e+03   \n","mean   3.123889e-17 -1.135960e-17 -9.087677e-17 -4.543838e-17  4.543838e-17   \n","std    1.000100e+00  1.000100e+00  1.000100e+00  1.000100e+00  1.000100e+00   \n","min   -1.680735e+00 -7.897398e+00 -9.651785e-01 -1.245842e+00 -1.285284e+00   \n","25%   -8.488678e-01 -3.719513e-01 -6.860668e-01 -8.188598e-01 -8.184353e-01   \n","50%    8.231806e-02  5.065980e-02 -4.096615e-01 -2.836861e-01 -2.793412e-01   \n","75%    9.434047e-01  4.177781e-01  4.512094e-01  5.295972e-01  5.264773e-01   \n","max    1.384124e+00  2.915852e+00  5.817467e+00  2.776808e+00  2.780195e+00   \n","\n","          bb_lower           obv         stoch        ultosc  \n","count  5004.000000  5.004000e+03  5.004000e+03  5.004000e+03  \n","mean      0.000000 -9.087677e-17  1.943556e-17 -3.407879e-17  \n","std       1.000100  1.000100e+00  1.000100e+00  1.000100e+00  \n","min      -1.341035 -1.685169e+00 -2.596039e+00 -3.136567e+00  \n","25%      -0.812978 -1.026727e+00 -6.635020e-01 -6.987028e-01  \n","50%      -0.286394  1.837073e-01  1.651097e-02  4.740952e-02  \n","75%       0.537482  8.144148e-01  6.587921e-01  7.257772e-01  \n","max       2.789706  1.571345e+00  2.558240e+00  2.937593e+00  \n"]}],"source":["from gym.wrappers import StepAPICompatibility\n","\n","# Wrap your environment with the compatibility wrapper\n","trading_environment = gym.make('trading_agent-v0',\n","                               ticker='^GSPC',\n","                               trading_days=trading_days,\n","                               trading_cost_bps=trading_cost_bps,\n","                               time_cost_bps=time_cost_bps)\n","\n","# Add the compatibility wrapper\n","trading_environment = StepAPICompatibility(trading_environment)\n"]},{"cell_type":"code","execution_count":85,"id":"4ad87e7b-cdfe-4f14-a889-f380990a366e","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728242310590,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"4ad87e7b-cdfe-4f14-a889-f380990a366e"},"outputs":[],"source":["state_dim = trading_environment.observation_space.shape[0]\n","num_actions = trading_environment.action_space.n\n","max_episode_steps = trading_environment.spec.max_episode_steps"]},{"cell_type":"markdown","source":["# DDQNAgent Class\n","\n","The `DDQNAgent` class implements a **Double Deep Q-Network (DDQN)** to train an agent using reinforcement learning. The agent interacts with a trading environment, aiming to maximize cumulative rewards by choosing actions (e.g., buy, sell, hold) based on the market state.\n","\n","### Key Components:\n","1. **Online Network**: This neural network selects actions by estimating Q-values for each state-action pair. The network is trained to predict future rewards based on current observations.\n","\n","2. **Target Network**: A separate network used for action evaluation. It is updated periodically with the weights from the online network. By decoupling action selection from evaluation, DDQN reduces the overestimation bias inherent in standard DQN.\n","\n","3. **Experience Replay**: The agent stores experiences (state, action, reward, next state) in a replay buffer. During training, random samples (mini-batches) from the buffer are used to break the correlation between consecutive experiences, leading to more stable learning.\n","\n","4. **Epsilon-Greedy Policy**: The agent uses an epsilon-greedy policy to balance exploration and exploitation. It chooses a random action with probability $\\epsilon$ and the action with the highest Q-value with probability $1 - \\epsilon$. Epsilon decays over time, reducing exploration as the agent learns.\n","\n","5. **Training**: The agent is trained using mini-batch gradient descent. The loss is the difference between the predicted Q-values from the online network and the target Q-values, computed using the target network. The loss is minimized to improve the Q-value estimations over time.\n"],"metadata":{"id":"sJi-dKqm95WR"},"id":"sJi-dKqm95WR"},{"cell_type":"code","execution_count":86,"id":"36106286-7f78-417d-8d09-6ca80bce232d","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728242310590,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"36106286-7f78-417d-8d09-6ca80bce232d"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import numpy as np\n","from collections import deque\n","import logging\n","\n","# Set up logging configuration\n","logging.basicConfig(level=logging.DEBUG)\n","\n","class DDQNAgent:\n","    def __init__(self, state_dim,\n","                 num_actions,\n","                 learning_rate,\n","                 gamma,\n","                 epsilon_start,\n","                 epsilon_end,\n","                 epsilon_decay_steps,\n","                 epsilon_exponential_decay,\n","                 replay_capacity,\n","                 architecture,\n","                 l2_reg,\n","                 tau,\n","                 batch_size):\n","        # Initialize parameters for the DDQN agent\n","        self.state_dim = state_dim\n","        self.num_actions = num_actions\n","        self.experience = deque([], maxlen=replay_capacity)  # Replay buffer\n","        self.learning_rate = learning_rate\n","        self.gamma = gamma\n","        self.architecture = architecture\n","        self.l2_reg = l2_reg\n","\n","        # Set device to GPU if available, otherwise use CPU\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Build the online and target networks, and move them to the correct device\n","        self.online_network = self.build_model().to(self.device)\n","        self.target_network = self.build_model().to(self.device)\n","        self.update_target()\n","\n","        # Initialize epsilon for epsilon-greedy action selection\n","        self.epsilon = epsilon_start\n","        self.epsilon_decay_steps = epsilon_decay_steps\n","        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n","        self.epsilon_exponential_decay = epsilon_exponential_decay\n","        self.epsilon_history = []\n","\n","        # Track metrics for episodes, steps, rewards, and training\n","        self.total_steps = 0\n","        self.episodes = 0\n","        self.episode_length = 0\n","        self.rewards_history = []\n","        self.steps_per_episode = []\n","        self.episode_reward = 0\n","\n","        # Set batch size and target network update frequency (tau)\n","        self.batch_size = batch_size\n","        self.tau = tau\n","        self.losses = []\n","        self.train = True\n","\n","        # Optimizer and loss function\n","        self.optimizer = optim.Adam(self.online_network.parameters(), lr=self.learning_rate)\n","        self.criterion = nn.MSELoss()\n","\n","    def build_model(self):\n","        \"\"\"\n","        Build the neural network model based on the architecture provided.\n","        The model will be a sequential feed-forward network with ReLU activation.\n","        \"\"\"\n","        layers = []\n","        input_dim = self.state_dim\n","\n","        for units in self.architecture:\n","            layers.append(nn.Linear(input_dim, units))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.Dropout(0.1))  # Add dropout for regularization\n","            input_dim = units\n","\n","        # Output layer to map to num_actions\n","        layers.append(nn.Linear(input_dim, self.num_actions))\n","\n","        return nn.Sequential(*layers)\n","\n","    def update_target(self):\n","        \"\"\"\n","        Copy the weights from the online network to the target network.\n","        This is done periodically to stabilize training.\n","        \"\"\"\n","        self.target_network.load_state_dict(self.online_network.state_dict())\n","\n","    def epsilon_greedy_policy(self, state):\n","        \"\"\"\n","        Select an action using epsilon-greedy policy.\n","        With probability epsilon, choose a random action (exploration).\n","        Otherwise, choose the action with the highest Q-value (exploitation).\n","        \"\"\"\n","        self.total_steps += 1\n","\n","        # Explore: select a random action\n","        if random.random() <= self.epsilon:\n","            return random.choice(range(self.num_actions))  # Random action (exploration)\n","\n","        # Exploit: select the action with the highest Q-value\n","        state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)  # Add batch dimension\n","\n","        # Get Q-values from the online network\n","        with torch.no_grad():  # No gradient computation during inference\n","            q_values = self.online_network(state_tensor).cpu().numpy()  # Get Q-values and convert to NumPy\n","\n","        # Select the action with the highest Q-value\n","        action = np.argmax(q_values)  # Get index of the max Q-value\n","        return action  # Return the action as an integer\n","\n","    def memorize_transition(self, s, a, r, s_prime, not_done):\n","        \"\"\"\n","        Store a transition (s, a, r, s') in the replay buffer.\n","        If the episode ends (done), handle epsilon decay and reset episode metrics.\n","        \"\"\"\n","        if not_done:\n","            self.episode_reward += r\n","            self.episode_length += 1\n","        else:\n","            if self.train:\n","                if self.episodes < self.epsilon_decay_steps:\n","                    self.epsilon -= self.epsilon_decay\n","                else:\n","                    self.epsilon *= self.epsilon_exponential_decay\n","\n","            self.episodes += 1\n","            self.rewards_history.append(self.episode_reward)\n","            self.steps_per_episode.append(self.episode_length)\n","            self.episode_reward, self.episode_length = 0, 0\n","\n","        self.experience.append((s, a, r, s_prime, not_done))\n","\n","    def experience_replay(self):\n","        \"\"\"\n","        Train the online network using experience replay.\n","        Sample a random batch of transitions from the replay buffer and update the network.\n","        \"\"\"\n","        if len(self.experience) < self.batch_size:\n","            logging.debug(\"Not enough experiences to sample.\")\n","            return\n","\n","        # Sample a minibatch from the replay buffer\n","        minibatch = random.sample(self.experience, self.batch_size)\n","        states, actions, rewards, next_states, not_done = zip(*minibatch)\n","\n","        # Convert experience data into PyTorch tensors and move them to the correct device\n","        states = torch.FloatTensor(states).to(self.device)\n","        actions = torch.LongTensor(actions).to(self.device)\n","        rewards = torch.FloatTensor(rewards).to(self.device).view(-1)  # Shape: [batch_size]\n","        next_states = torch.FloatTensor(next_states).to(self.device)\n","        not_done = torch.FloatTensor(not_done).to(self.device).view(-1)  # Shape: [batch_size]\n","\n","        # Debugging statements to check shapes\n","        logging.debug(\"Shapes before target calculation:\")\n","        logging.debug(f\"rewards: {rewards.shape}, not_done: {not_done.shape}\")\n","\n","        # Compute Q-values for the next states using the online network\n","        next_q_values = self.online_network(next_states)\n","\n","        # Get the best actions for the next states\n","        best_actions = next_q_values.argmax(dim=1)\n","\n","        # Compute Q-values for the next states using the target network\n","        next_q_values_target = self.target_network(next_states)\n","\n","        # Gather target Q-values using the actions from next states\n","        target_q_values = next_q_values_target.gather(1, best_actions.unsqueeze(1)).squeeze(1)  # Shape: [batch_size]\n","\n","        # Debugging statement to check the shape of target_q_values\n","        logging.debug(f\"target_q_values shape: {target_q_values.shape}\")\n","\n","        # Compute the target Q-value: r + gamma * max(Q(s', a'))\n","        targets = rewards + not_done * self.gamma * target_q_values\n","\n","        # Compute Q-values for the current states using the online network\n","        q_values = self.online_network(states)\n","\n","        # Gather the Q-values corresponding to the actions taken\n","        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1) # Select Q-values for actions taken\n","\n","\n","        # Compute the loss (difference between predicted and target Q-values)\n","        loss = self.criterion(q_values, targets.detach())\n","\n","        # Backpropagate the loss and\n","        # Backpropagate the loss and update the network weights\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # Track the loss for analysis\n","        self.losses.append(loss.item())\n","\n","        # Periodically update the target network based on tau\n","        if self.total_steps % self.tau == 0:\n","            self.update_target()\n"]},{"cell_type":"markdown","source":["# Set Hyperparameters\n","1. **Discount Factor (`gamma`)**:\n","   - **Value**: `0.99`\n","   - Controls how much the agent values future rewards compared to immediate rewards. A value close to 1 favors long-term rewards over short-term gains.\n","\n","2. **Target Network Update Frequency (`tau`)**:\n","   - **Value**: `100`\n","   - Defines how often the target network is updated with the online networkâ€™s weights. This helps stabilize the learning process.\n","\n","3. **Neural Network Architecture**:\n","   - **Value**: `(256, 256)`\n","   - Specifies the number of units (neurons) in each hidden layer of the network. In this case, two hidden layers with 256 units each are used to represent the Q-values.\n","\n","4. **Learning Rate (`learning_rate`)**:\n","   - **Value**: `0.0001`\n","   - Determines the step size for updating the network during training. A smaller value ensures gradual learning and helps avoid large fluctuations in weight updates.\n","\n","5. **L2 Regularization (`l2_reg`)**:\n","   - **Value**: `1e-6`\n","   - Helps prevent overfitting by penalizing large weights in the network, which can lead to better generalization to unseen data.\n","\n","6. **Experience Replay Capacity (`replay_capacity`)**:\n","   - **Value**: `1e6`\n","   - Defines the maximum size of the replay buffer that stores past experiences (state, action, reward, next state, done). Experiences are sampled from this buffer during training.\n","\n","7. **Batch Size (`batch_size`)**:\n","   - **Value**: `4096`\n","   - Specifies the number of experiences sampled from the replay buffer for each training step. A larger batch size can lead to more stable learning.\n","\n","8. **Epsilon-Greedy Policy**:\n","   - **`epsilon_start`**: Initial exploration value (`1.0`). High exploration at the start of training.\n","   - **`epsilon_end`**: Final exploration value (`0.01`). Low exploration after the agent has learned from its environment.\n","   - **`epsilon_decay_steps`**: Number of steps over which `epsilon` decays linearly from `epsilon_start` to `epsilon_end` (250 steps).\n","   - **`epsilon_exponential_decay`**: An additional exponential decay factor (`0.99`) applied to epsilon to further fine-tune the balance between exploration and exploitation."],"metadata":{"id":"SEcCCj2Sn3uZ"},"id":"SEcCCj2Sn3uZ"},{"cell_type":"code","execution_count":87,"id":"4aOMuo8AVLtm","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728242310590,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"4aOMuo8AVLtm"},"outputs":[],"source":["gamma = .99  # discount factor\n","tau = 100  # target network update frequency\n","#NN Architecture\n","architecture = (256, 256)  # units per layer\n","learning_rate = 0.0001  # learning rate\n","l2_reg = 1e-6  # L2 regularization\n","#Experience Replay\n","replay_capacity = int(1e6)\n","batch_size = 4096\n","#greedy Policy\n","epsilon_start = 1.0\n","epsilon_end = .01\n","epsilon_decay_steps = 250\n","epsilon_exponential_decay = .99"]},{"cell_type":"code","execution_count":88,"id":"6tzwra2KVfGf","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728242310590,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"6tzwra2KVfGf"},"outputs":[],"source":["ddqn = DDQNAgent(state_dim=state_dim,\n","                 num_actions=num_actions,\n","                 learning_rate=learning_rate,\n","                 gamma=gamma,\n","                 epsilon_start=epsilon_start,\n","                 epsilon_end=epsilon_end,\n","                 epsilon_decay_steps=epsilon_decay_steps,\n","                 epsilon_exponential_decay=epsilon_exponential_decay,\n","                 replay_capacity=replay_capacity,\n","                 architecture=architecture,\n","                 l2_reg=l2_reg,\n","                 tau=tau,\n","                 batch_size=batch_size)"]},{"cell_type":"code","execution_count":89,"id":"ZU9c1hndVuhf","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728242310590,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"ZU9c1hndVuhf"},"outputs":[],"source":["total_steps = 0\n","max_episodes = 800\n","#Initialize variables\n","episode_time, navs, market_navs, diffs, episode_eps = [], [], [], [], []\n","#Visualization\n","def track_results(episode, nav_ma_100, nav_ma_10,\n","                  market_nav_100, market_nav_10,\n","                  win_ratio, total, epsilon):\n","    time_ma = np.mean([episode_time[-100:]])\n","    T = np.sum(episode_time)\n","\n","    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n","    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n","    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n","    print(template.format(episode, format_time(total),\n","                          nav_ma_100-1, nav_ma_10-1,\n","                          market_nav_100-1, market_nav_10-1,\n","                          win_ratio, epsilon))"]},{"cell_type":"code","execution_count":90,"id":"qxsMrk0MVnYg","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3239095,"status":"ok","timestamp":1728253472086,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"qxsMrk0MVnYg","collapsed":true,"outputId":"d62968c6-5736-476e-ac75-2d74c34ac73a"},"outputs":[{"output_type":"stream","name":"stdout","text":["  10 | 00:00:01 | Agent:  -4.0% ( -4.0%) | Market:   6.3% (  6.3%) | Wins: 20.0% | eps:  0.960\n","  20 | 00:00:32 | Agent:  -0.6% (  2.9%) | Market:  11.8% ( 17.3%) | Wins: 25.0% | eps:  0.921\n","  30 | 00:01:56 | Agent:  -0.1% (  0.8%) | Market:  12.5% ( 13.8%) | Wins: 20.0% | eps:  0.881\n","  40 | 00:03:21 | Agent:  -0.7% ( -2.4%) | Market:  12.5% ( 12.6%) | Wins: 20.0% | eps:  0.842\n","  50 | 00:04:48 | Agent:  -2.2% ( -8.1%) | Market:  11.5% (  7.8%) | Wins: 20.0% | eps:  0.802\n","  60 | 00:06:15 | Agent:  -0.9% (  5.7%) | Market:  12.0% ( 14.3%) | Wins: 21.7% | eps:  0.762\n","  70 | 00:07:44 | Agent:   0.2% (  6.7%) | Market:  11.9% ( 11.5%) | Wins: 24.3% | eps:  0.723\n","  80 | 00:09:17 | Agent:   0.7% (  4.3%) | Market:  12.6% ( 17.2%) | Wins: 22.5% | eps:  0.683\n","  90 | 00:10:51 | Agent:   0.6% ( -0.7%) | Market:  12.6% ( 13.1%) | Wins: 23.3% | eps:  0.644\n"," 100 | 00:12:26 | Agent:   0.3% ( -2.0%) | Market:  12.0% (  6.2%) | Wins: 22.0% | eps:  0.604\n"," 110 | 00:14:02 | Agent:   1.2% (  4.8%) | Market:  11.0% ( -3.9%) | Wins: 26.0% | eps:  0.564\n"," 120 | 00:15:40 | Agent:   0.9% ( -0.4%) | Market:  10.4% ( 11.5%) | Wins: 25.0% | eps:  0.525\n"," 130 | 00:17:18 | Agent:   1.4% (  6.0%) | Market:   9.8% (  8.1%) | Wins: 29.0% | eps:  0.485\n"," 140 | 00:18:59 | Agent:   1.6% ( -0.7%) | Market:   9.5% (  9.7%) | Wins: 30.0% | eps:  0.446\n"," 150 | 00:20:40 | Agent:   1.1% (-13.2%) | Market:  10.0% ( 12.5%) | Wins: 29.0% | eps:  0.406\n"," 160 | 00:22:23 | Agent:  -0.1% ( -5.3%) | Market:   9.1% (  5.2%) | Wins: 28.0% | eps:  0.366\n"," 170 | 00:24:07 | Agent:   0.7% ( 14.3%) | Market:   8.4% (  4.8%) | Wins: 30.0% | eps:  0.327\n"," 180 | 00:25:51 | Agent:   0.1% ( -1.5%) | Market:   7.5% (  7.3%) | Wins: 31.0% | eps:  0.287\n"," 190 | 00:27:38 | Agent:   0.6% (  4.1%) | Market:   7.1% (  9.3%) | Wins: 33.0% | eps:  0.248\n"," 200 | 00:29:25 | Agent:   0.9% (  0.8%) | Market:   8.2% ( 17.4%) | Wins: 32.0% | eps:  0.208\n"," 210 | 00:31:14 | Agent:   0.3% ( -1.4%) | Market:  10.0% ( 13.7%) | Wins: 28.0% | eps:  0.168\n"," 220 | 00:33:04 | Agent:   0.2% ( -1.0%) | Market:  10.7% ( 18.5%) | Wins: 26.0% | eps:  0.129\n"," 230 | 00:34:56 | Agent:  -0.8% ( -3.6%) | Market:  10.9% ( 10.8%) | Wins: 21.0% | eps:  0.089\n"," 240 | 00:36:48 | Agent:  -0.4% (  2.4%) | Market:  11.1% ( 11.8%) | Wins: 20.0% | eps:  0.050\n"," 250 | 00:38:42 | Agent:   1.4% (  5.0%) | Market:  11.1% ( 12.0%) | Wins: 22.0% | eps:  0.010\n"," 260 | 00:40:38 | Agent:   2.7% (  7.6%) | Market:  11.4% (  8.0%) | Wins: 25.0% | eps:  0.009\n"," 270 | 00:42:36 | Agent:   2.1% (  8.3%) | Market:  12.0% ( 11.4%) | Wins: 21.0% | eps:  0.008\n"," 280 | 00:44:34 | Agent:   2.6% (  4.1%) | Market:  11.7% (  4.2%) | Wins: 25.0% | eps:  0.007\n"," 290 | 00:46:34 | Agent:   3.2% ( 10.2%) | Market:  11.9% ( 11.3%) | Wins: 25.0% | eps:  0.007\n"," 300 | 00:48:35 | Agent:   3.1% ( -1.1%) | Market:  12.3% ( 21.2%) | Wins: 27.0% | eps:  0.006\n"," 310 | 00:50:37 | Agent:   3.2% (  0.5%) | Market:  11.4% (  5.1%) | Wins: 27.0% | eps:  0.005\n"," 320 | 00:52:40 | Agent:   4.6% ( 12.7%) | Market:  10.5% (  8.9%) | Wins: 31.0% | eps:  0.005\n"," 330 | 00:54:45 | Agent:   5.9% (  9.8%) | Market:   9.8% (  4.1%) | Wins: 37.0% | eps:  0.004\n"," 340 | 00:56:51 | Agent:   7.6% ( 18.8%) | Market:   9.8% ( 11.8%) | Wins: 40.0% | eps:  0.004\n"," 350 | 00:58:58 | Agent:   9.8% ( 27.7%) | Market:   8.0% ( -5.4%) | Wins: 45.0% | eps:  0.004\n"," 360 | 01:01:07 | Agent:  11.1% ( 20.1%) | Market:   9.4% ( 21.9%) | Wins: 45.0% | eps:  0.003\n"," 370 | 01:03:16 | Agent:  11.4% ( 11.2%) | Market:   8.8% (  5.0%) | Wins: 50.0% | eps:  0.003\n"," 380 | 01:05:28 | Agent:  11.7% (  7.0%) | Market:   9.6% ( 12.6%) | Wins: 48.0% | eps:  0.003\n"," 390 | 01:07:41 | Agent:  11.6% (  9.3%) | Market:   9.6% ( 11.4%) | Wins: 46.0% | eps:  0.002\n"," 400 | 01:09:56 | Agent:  12.9% ( 12.2%) | Market:   9.0% ( 14.4%) | Wins: 48.0% | eps:  0.002\n"," 410 | 01:12:12 | Agent:  15.5% ( 26.5%) | Market:   9.1% (  6.1%) | Wins: 53.0% | eps:  0.002\n"," 420 | 01:14:31 | Agent:  15.4% ( 11.2%) | Market:   9.4% ( 12.5%) | Wins: 53.0% | eps:  0.002\n"," 430 | 01:16:49 | Agent:  15.1% (  6.7%) | Market:   9.2% (  1.7%) | Wins: 52.0% | eps:  0.002\n"," 440 | 01:19:10 | Agent:  13.8% (  6.2%) | Market:   8.0% ( -0.1%) | Wins: 52.0% | eps:  0.001\n"," 450 | 01:21:32 | Agent:  11.3% (  2.7%) | Market:   9.4% (  8.7%) | Wins: 47.0% | eps:  0.001\n"," 460 | 01:23:56 | Agent:  11.0% ( 17.1%) | Market:   8.2% (  9.2%) | Wins: 48.0% | eps:  0.001\n"," 470 | 01:26:21 | Agent:  10.7% (  8.5%) | Market:   8.0% (  3.0%) | Wins: 47.0% | eps:  0.001\n"," 480 | 01:28:49 | Agent:  10.9% (  8.3%) | Market:   7.9% ( 12.0%) | Wins: 46.0% | eps:  0.001\n"," 490 | 01:31:19 | Agent:  11.5% ( 15.9%) | Market:   6.5% ( -2.3%) | Wins: 50.0% | eps:  0.001\n"," 500 | 01:33:50 | Agent:  10.9% (  5.7%) | Market:   6.1% ( 10.3%) | Wins: 48.0% | eps:  0.001\n"," 510 | 01:36:24 | Agent:   9.0% (  7.9%) | Market:   6.3% (  8.2%) | Wins: 45.0% | eps:  0.001\n"," 520 | 01:38:59 | Agent:   8.3% (  4.0%) | Market:   5.3% (  2.1%) | Wins: 46.0% | eps:  0.001\n"," 530 | 01:41:37 | Agent:   8.2% (  5.6%) | Market:   7.0% ( 19.3%) | Wins: 43.0% | eps:  0.001\n"," 540 | 01:44:17 | Agent:   8.6% (  9.9%) | Market:   6.6% ( -4.3%) | Wins: 45.0% | eps:  0.001\n"," 550 | 01:46:59 | Agent:   8.8% (  4.9%) | Market:   6.5% (  7.3%) | Wins: 45.0% | eps:  0.000\n"," 560 | 01:49:43 | Agent:   7.7% (  6.4%) | Market:   6.7% ( 11.3%) | Wins: 43.0% | eps:  0.000\n"," 570 | 01:52:29 | Agent:   8.1% ( 12.5%) | Market:   6.8% (  4.7%) | Wins: 45.0% | eps:  0.000\n"," 580 | 01:55:18 | Agent:   8.5% ( 11.9%) | Market:   6.1% (  4.2%) | Wins: 49.0% | eps:  0.000\n"," 590 | 01:58:08 | Agent:   7.9% ( 10.5%) | Market:   7.1% (  8.0%) | Wins: 47.0% | eps:  0.000\n"," 600 | 02:01:01 | Agent:   8.4% ( 11.0%) | Market:   7.7% ( 16.8%) | Wins: 48.0% | eps:  0.000\n"," 610 | 02:03:56 | Agent:   7.8% (  1.3%) | Market:   7.7% (  7.7%) | Wins: 48.0% | eps:  0.000\n"," 620 | 02:06:53 | Agent:   8.3% (  9.3%) | Market:   8.2% (  6.8%) | Wins: 49.0% | eps:  0.000\n"," 630 | 02:09:53 | Agent:   9.0% ( 12.9%) | Market:   7.7% ( 14.3%) | Wins: 50.0% | eps:  0.000\n"," 640 | 02:12:54 | Agent:   9.3% ( 12.3%) | Market:   9.9% ( 18.2%) | Wins: 45.0% | eps:  0.000\n"," 650 | 02:15:56 | Agent:   9.5% (  7.4%) | Market:  10.1% (  8.6%) | Wins: 45.0% | eps:  0.000\n"," 660 | 02:19:00 | Agent:   8.9% ( -0.1%) | Market:  10.1% ( 11.4%) | Wins: 42.0% | eps:  0.000\n"," 670 | 02:22:07 | Agent:  10.5% ( 28.5%) | Market:   9.7% (  0.8%) | Wins: 40.0% | eps:  0.000\n"," 680 | 02:25:15 | Agent:   9.6% (  3.2%) | Market:   9.8% (  5.6%) | Wins: 36.0% | eps:  0.000\n"," 690 | 02:28:27 | Agent:   8.9% (  3.4%) | Market:  10.3% ( 12.5%) | Wins: 34.0% | eps:  0.000\n"," 700 | 02:31:40 | Agent:   8.5% (  7.3%) | Market:   9.7% ( 11.5%) | Wins: 35.0% | eps:  0.000\n"," 710 | 02:34:56 | Agent:   9.1% (  7.4%) | Market:   9.8% (  7.8%) | Wins: 36.0% | eps:  0.000\n"," 720 | 02:38:14 | Agent:  10.3% ( 20.3%) | Market:  10.0% (  8.7%) | Wins: 35.0% | eps:  0.000\n"," 730 | 02:41:34 | Agent:   9.0% ( -0.1%) | Market:   9.5% (  9.5%) | Wins: 34.0% | eps:  0.000\n"," 740 | 02:44:56 | Agent:   8.3% (  6.2%) | Market:   9.0% ( 13.2%) | Wins: 35.0% | eps:  0.000\n"," 750 | 02:48:21 | Agent:   9.4% ( 17.9%) | Market:   8.8% (  6.8%) | Wins: 36.0% | eps:  0.000\n"," 760 | 02:51:48 | Agent:  11.0% ( 16.1%) | Market:   8.7% ( 10.1%) | Wins: 40.0% | eps:  0.000\n"," 770 | 02:55:18 | Agent:   9.7% ( 15.2%) | Market:   9.0% (  4.1%) | Wins: 40.0% | eps:  0.000\n"," 780 | 02:58:49 | Agent:  10.3% (  9.1%) | Market:   8.2% ( -2.7%) | Wins: 41.0% | eps:  0.000\n"," 790 | 03:02:24 | Agent:  11.7% ( 17.2%) | Market:   8.0% ( 11.4%) | Wins: 43.0% | eps:  0.000\n"," 800 | 03:06:01 | Agent:  12.7% ( 17.6%) | Market:   7.7% (  7.9%) | Wins: 46.0% | eps:  0.000\n"]}],"source":["# Train Agent\n","start = time()\n","results = []\n","for episode in range(1, max_episodes + 1):\n","    this_state = trading_environment.reset()\n","    logging.debug(f\"Initial state: {this_state}\")\n","\n","    for episode_step in range(max_episode_steps):\n","        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n","        logging.debug(f\"Action taken: {action}\")\n","\n","        next_state, reward, done, _ = trading_environment.step(action)\n","        logging.debug(f\"Next state: {next_state}, Reward: {reward}, Done: {done}\")\n","\n","        ddqn.memorize_transition(this_state,\n","                                 action,\n","                                 reward,\n","                                 next_state,\n","                                 0.0 if done else 1.0)\n","        if ddqn.train:\n","            ddqn.experience_replay()\n","\n","        if done:\n","            break\n","        this_state = next_state\n","\n","    # get DataFrame with sequence of actions, returns, and nav values\n","    result = trading_environment.env.simulator.results()\n","\n","    # get results of last step\n","    final = result.iloc[-2]\n","    logging.debug(f\"Final result of episode {episode}: {final}\")\n","\n","    # apply return (net of cost) of last action to last starting nav\n","    nav = final.nav * (1 + final.strategy_return)\n","    navs.append(nav)\n","\n","    # market nav\n","    market_nav = final.market_nav\n","    market_navs.append(market_nav)\n","\n","    # track difference between agent and market NAV results\n","    diff = nav - market_nav\n","    diffs.append(diff)\n","\n","    if episode % 10 == 0:\n","        track_results(episode,\n","                      np.mean(navs[-100:]),\n","                      np.mean(navs[-10:]),\n","                      np.mean(market_navs[-100:]),\n","                      np.mean(market_navs[-10:]),\n","                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n","                      time() - start, ddqn.epsilon)\n","    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n","        print(result.tail())\n","        break\n","\n","trading_environment.close()\n","\n","\n"]},{"cell_type":"code","source":["# Ensure that all lists are the same length as the episode count\n","min_length = min(len(navs), len(market_navs), len(diffs), episode)\n","\n","# Truncate lists to match the smallest length if necessary\n","navs = navs[:min_length]\n","market_navs = market_navs[:min_length]\n","diffs = diffs[:min_length]\n","\n","# Construct the DataFrame\n","final_results = pd.DataFrame({\n","    'Episode': list(range(1, min_length + 1)),\n","    'Agent': navs,\n","    'Market': market_navs,\n","    'Difference': diffs\n","}).set_index('Episode')"],"metadata":{"id":"hoJjK2NulDUR","executionInfo":{"status":"ok","timestamp":1728253472087,"user_tz":-60,"elapsed":4,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"}}},"id":"hoJjK2NulDUR","execution_count":91,"outputs":[]},{"cell_type":"code","source":["final_results.to_csv(results_path / 'final_results.csv', index=False)"],"metadata":{"id":"jZ5j4U8HlYIq","executionInfo":{"status":"ok","timestamp":1728253472088,"user_tz":-60,"elapsed":4,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"}}},"id":"jZ5j4U8HlYIq","execution_count":92,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"Zg9JTt1KNGZH","executionInfo":{"status":"ok","timestamp":1728253472089,"user_tz":-60,"elapsed":4,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"}}},"id":"Zg9JTt1KNGZH","execution_count":93,"outputs":[]},{"cell_type":"code","execution_count":70,"id":"J5ANTS2UGH2r","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1805,"status":"ok","timestamp":1728242308081,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"},"user_tz":-60},"id":"J5ANTS2UGH2r","outputId":"e2d4e0bf-c79d-4c6c-f28b-640feac7e2c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  4059    0  4059    0     0  12978      0 --:--:-- --:--:-- --:--:-- 13009\n","100  517k  100  517k    0     0   830k      0 --:--:-- --:--:-- --:--:-- 4535k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  4063    0  4063    0     0   9052      0 --:--:-- --:--:-- --:--:--  9069\n","100  392k  100  392k    0     0   510k      0 --:--:-- --:--:-- --:--:-- 86.1M\n"]}],"source":["#Only uncomment the below if you need to install the Talib library\n","\n","\n","#url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'\n","#!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1\n","#url = 'https://anaconda.org/conda-forge/ta-lib/0.4.19/download/linux-64/ta-lib-0.4.19-py310hde88566_4.tar.bz2'\n","#!curl -L $url | tar xj -C /usr/local/lib/python3.10/dist-packages/ lib/python3.10/site-packages/talib --strip-components=3\n","#import talib"]},{"cell_type":"code","source":[],"metadata":{"id":"AOpL5h0X-2Rl","executionInfo":{"status":"ok","timestamp":1728253472089,"user_tz":-60,"elapsed":4,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"}}},"id":"AOpL5h0X-2Rl","execution_count":93,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nrCvv_F7_pg6","executionInfo":{"status":"ok","timestamp":1728253472090,"user_tz":-60,"elapsed":5,"user":{"displayName":"Aglaia Vasileiou","userId":"07251251541452201680"}}},"id":"nrCvv_F7_pg6","execution_count":93,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1lyy2ms_6XXdi4G8cEZOZERsz5Y1Eq7QC","timestamp":1727888171977}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
